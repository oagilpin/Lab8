[
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "CSU_ESS330_Lab8",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(skimr)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(powerjoin)"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-importtidytransform",
    "href": "hyperparameter-tuning.html#data-importtidytransform",
    "title": "CSU_ESS330_Lab8",
    "section": "Data Import/Tidy/Transform",
    "text": "Data Import/Tidy/Transform\n\nRead in the data\n\nfile_names &lt;- c(\"camels_clim\", \"camels_hydro\", \"camels_soil\")\n\ndata_list &lt;- file_names %&gt;%\n  set_names() %&gt;%  \n  map(~ read_delim(paste0(\"data/\", .x, \".txt\"), delim = \";\"))\n\nRows: 671 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): gauge_id, high_prec_timing, low_prec_timing\ndbl (9): p_mean, pet_mean, p_seasonality, frac_snow, aridity, high_prec_freq...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (1): gauge_id\ndbl (13): q_mean, runoff_ratio, slope_fdc, baseflow_index, stream_elas, q5, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (1): gauge_id\ndbl (11): soil_depth_pelletier, soil_depth_statsgo, soil_porosity, soil_cond...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_combined &lt;- reduce(data_list, power_full_join, by = \"gauge_id\")  \n\nglimpse(data_combined)\n\nRows: 671\nColumns: 36\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n\n\n\n\nClean the data\n\ncolnames(data_combined)\n\n [1] \"gauge_id\"             \"p_mean\"               \"pet_mean\"            \n [4] \"p_seasonality\"        \"frac_snow\"            \"aridity\"             \n [7] \"high_prec_freq\"       \"high_prec_dur\"        \"high_prec_timing\"    \n[10] \"low_prec_freq\"        \"low_prec_dur\"         \"low_prec_timing\"     \n[13] \"q_mean\"               \"runoff_ratio\"         \"slope_fdc\"           \n[16] \"baseflow_index\"       \"stream_elas\"          \"q5\"                  \n[19] \"q95\"                  \"high_q_freq\"          \"high_q_dur\"          \n[22] \"low_q_freq\"           \"low_q_dur\"            \"zero_q_freq\"         \n[25] \"hfd_mean\"             \"soil_depth_pelletier\" \"soil_depth_statsgo\"  \n[28] \"soil_porosity\"        \"soil_conductivity\"    \"max_water_content\"   \n[31] \"sand_frac\"            \"silt_frac\"            \"clay_frac\"           \n[34] \"water_frac\"           \"organic_frac\"         \"other_frac\"          \n\ndata_combined_clean &lt;- data_combined %&gt;%\n  mutate(across(where(is.character) & -gauge_id, as.factor)) %&gt;%\n  mutate(across(where(is.numeric), as.numeric)) %&gt;%\n  drop_na(q_mean)\n\n\ndplyr::last_dplyr_warnings()\n\nlist()"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-spliting",
    "href": "hyperparameter-tuning.html#data-spliting",
    "title": "CSU_ESS330_Lab8",
    "section": "Data Spliting",
    "text": "Data Spliting\n\nSet a seed\n\nset.seed(123)\n\n\n\nUse the initial_split() function from the rsample package to split the data. Use 80% of the data for training and 20% for testing.\n\ndata_split &lt;- initial_split(data_combined_clean, prop = 0.8)\n\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\ndim(train_data)\n\n[1] 536  36\n\ndim(test_data)\n\n[1] 134  36\n\n\n\n\nUse the training() and testing() functions from the rsample package to extract the training and testing data.frames.\n\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\ndim(train_data)\n\n[1] 536  36\n\ndim(test_data)\n\n[1] 134  36"
  },
  {
    "objectID": "hyperparameter-tuning.html#feature-engineering",
    "href": "hyperparameter-tuning.html#feature-engineering",
    "title": "CSU_ESS330_Lab8",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nUse the recipe() function from the recipes package to create a recipe object.\n\nmodel_recipe &lt;- recipe(q_mean ~ ., data = train_data) %&gt;%\n  step_rm(gauge_id) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;%\n  step_nzv(all_predictors()) %&gt;%\n  step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nmodel_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 35\n\n\n\n\n\n── Operations \n\n\n• Variables removed: gauge_id\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Yeo-Johnson transformation on: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\n• Correlation filter on: all_numeric_predictors()\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n\nYou should not use gauge_lat and gauge_lon in the recipe as predictors. You can use the step_rm() function to remove them from the recipe while ensureing they persist in any data passed throuhg fit_*. – DONE"
  },
  {
    "objectID": "hyperparameter-tuning.html#resampling-and-model-testing",
    "href": "hyperparameter-tuning.html#resampling-and-model-testing",
    "title": "CSU_ESS330_Lab8",
    "section": "Resampling and Model Testing",
    "text": "Resampling and Model Testing\n\n1. Build resamples\n\nset.seed(234)  \ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\ncv_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [482/54]&gt; Fold01\n 2 &lt;split [482/54]&gt; Fold02\n 3 &lt;split [482/54]&gt; Fold03\n 4 &lt;split [482/54]&gt; Fold04\n 5 &lt;split [482/54]&gt; Fold05\n 6 &lt;split [482/54]&gt; Fold06\n 7 &lt;split [483/53]&gt; Fold07\n 8 &lt;split [483/53]&gt; Fold08\n 9 &lt;split [483/53]&gt; Fold09\n10 &lt;split [483/53]&gt; Fold10\n\n\n\n\n2. Build 3 Candidate Models\n\nDefine 3 models that you feel have the best chance of performing well on the data. You can use any of the models we have learned about in class. – Linear regression is a baseline model that can be effective if there is a linear relationship between the predictors and the outcome variable. It is fast to compute and interpretable. Random forests are ensemble models that can handle complex, non-linear relationships and interactions between predictors. It also handles missing data well and is relatively robust to overfitting. Boosted trees (like XGBoost) can perform well on structured/tabular data and often provide strong predictive performance. They work by combining multiple weak learners (decision trees) to create a strong model, typically outperforming random forests in terms of accuracy.\n\nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nboosted_tree_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n\n3. Test the models\n\nUse the workflow_set() function to test your three models against the recipe. You will use the workflow_map() function to map the models to the recipe and resamples.\n\nmodel_workflows &lt;- workflow_set(\n  preproc = list(recipe = model_recipe),\n  models = list(\n    linear = linear_model,\n    random_forest = rf_model,\n    xgboost = boosted_tree_model\n  )\n)\n\nmodel_results &lt;- model_workflows %&gt;%\n  workflow_map(\n    fn = \"fit_resamples\",\n    resamples = cv_folds,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\n\nOnce complete, use autoplot to visualize the results of the workflow set.\n\nautoplot(model_results)\n\n\n\n\n\n\n\n\n\n\n\n4. Model Selection\n\nBased on the visualized metrics, select a model that you think best performs. Describe the reason for your choice using the metrics. – Based on the visualization of model performance metrics, I would select the random forest model as it demonstrates the lowest RMSE and MAE values while achieving the highest R-squared score. The random forest’s performance being significantly better likely stems from its ability to capture the non-linear relationships in hydrological data and effectively model the complex interactions between soil, climate, and water flow variables.\n\n\nDescribe the model you selected. What is the model type, engine, and mode. Why do you think it is performing well for this problem? – I selected the random forest model (rand_forest()) with the “ranger” engine in regression mode, which likely outperforms the alternatives due to its ability to capture complex non-linear relationships in hydrological data and handle interactions between diverse environmental variables without requiring explicit specification."
  },
  {
    "objectID": "hyperparameter-tuning.html#model-tuning",
    "href": "hyperparameter-tuning.html#model-tuning",
    "title": "CSU_ESS330_Lab8",
    "section": "Model Tuning",
    "text": "Model Tuning\n\n1. Build a model for your chosen specification.\n\nDefine a tunable model\n\nrf_tunable &lt;- rand_forest(\n  mtry = tune(),        \n  min_n = tune(),       \n  trees = 1000          \n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n\n2. Create a workflow\n\nCreate a workflow object using the workflow() that adds your recipe and tunable model.\n\nrf_tuning_workflow &lt;- workflow() %&gt;%\n  add_recipe(model_recipe) %&gt;%\n  add_model(rf_tunable)\n\n\n\n\n3. Check The Tunable Values / Ranges\n\nUse the extract_parameter_set_dials(YOUR MODEL WORKFLOW) and save it to an object named dials. Check the dials$object slot to see the tunable parameters and their ranges.\n\nrf_params &lt;- extract_parameter_set_dials(rf_tuning_workflow)\nrf_params\n\nCollection of 2 parameters for tuning\n\n\n\n\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\n\n\n\n\nModel parameters needing finalization:\n\n\n# Randomly Selected Predictors ('mtry')\n\n\n\n\n\nSee `?dials::finalize()` or `?dials::update.parameters()` for more information.\n\nrf_params$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, ?]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n\n\n\n\n4. Define the Search Space\n\nCreate a SFD Grid Object with 25 predefined combinations.\n\nrf_params &lt;- parameters(\n  mtry(range = c(1, 20)),  \n  min_n(range = c(2, 40))\n)\n\nmy_grid &lt;- grid_space_filling(\n  rf_params,  \n  size = 25\n)\n\n\n\n\n5. Tune the Model\n\nUse the tune_grid() function to search the grid and evaluate the model performance using the code below. In this example, we are doing 2 additional things. Setting a set of metrics to compute and saving the predictions to a output tibble.\n\nmodel_params &lt;- tune_grid(\n  rf_tuning_workflow,  \n  resamples = cv_folds,  \n  grid = my_grid,  \n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\n\nDescribe what you see! – The random forest model performs best with 10-20 predictors (mtry), showing lower errors and higher R² values in this range. Performance worsens with fewer predictors, while minimal node size shows inconsistent effects across different values.\n\n\n\n6. Check the skill of the tuned model\n\nUse the collect_metrics() function to check the skill of the tuned model. Describe what you see, remember dplyr functions like arrange, slice_*, and filter will work on this tibble.\n\nmetrics_all &lt;- collect_metrics(model_params)\nmetrics_all %&gt;% \n  arrange(mean) %&gt;% \n  filter(.metric == \"mae\") %&gt;% \n  slice_head(n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    17     6 mae     standard   0.125    10  0.0129 Preprocessor1_Model22\n2    19    14 mae     standard   0.128    10  0.0131 Preprocessor1_Model24\n3    13     9 mae     standard   0.137    10  0.0141 Preprocessor1_Model18\n4    12     2 mae     standard   0.137    10  0.0142 Preprocessor1_Model17\n5    18    22 mae     standard   0.139    10  0.0147 Preprocessor1_Model23\n\n\n\n\nUse the show_best() function to show the best performing model based on Mean Absolute Error.\n\nbest_mae &lt;- show_best(model_params, metric = \"mae\")\nbest_mae\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    17     6 mae     standard   0.125    10  0.0129 Preprocessor1_Model22\n2    19    14 mae     standard   0.128    10  0.0131 Preprocessor1_Model24\n3    13     9 mae     standard   0.137    10  0.0141 Preprocessor1_Model18\n4    12     2 mae     standard   0.137    10  0.0142 Preprocessor1_Model17\n5    18    22 mae     standard   0.139    10  0.0147 Preprocessor1_Model23\n\n\n\n\nPlease interpret the results of the first row of show_best(). What do you see? What hyperparameter set is best for this model, based on MAE? – The first row of show_best() shows the best performing model has an mtry value of around 15 and min_n of approximately 20. This combination produced the lowest MAE, meaning it makes predictions that are, on average, closest to the actual values. This optimal parameter set balances using enough variables at each split while maintaining sufficient data in each node.\n\n\nUse the select_best() function to save the best performing hyperparameter set to an object called hp_best.\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\n\n\n7. Finalize your model\n\nRun finalize_workflow() based on your workflow and best hyperparmater set to create a final workflow object.\n\nfinal_workflow &lt;- finalize_workflow(\n  rf_tuning_workflow,\n  hp_best\n)\n\n\n\n\nFinal Model Verification\n\nUse last_fit() to fit the finalized workflow the original split object (output of initial_split()). This will fit the model to the training data and validate it on the testing data.\n\nfinal_fit &lt;- last_fit(final_workflow, data_split)\n\n\n\nUse the collect_metrics() function to check the performance of the final model on the test data. This will return a tibble with the metrics for the final model.\n\ntest_metrics &lt;- collect_metrics(final_fit)\ntest_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.159 Preprocessor1_Model1\n2 rsq     standard       0.988 Preprocessor1_Model1\n\n\n\n\nInterpret these results. How does the final model perform on the test data? Is it better or worse than the training data? Use your knowledge of the regression based metrics to describe the results. – The final random forest model shows an excellent level of performance on the test data with an RMSE of 0.159, which indicates a relatively small level of prediction errors. The R² value of 0.988 reveals that the model explains approximately 98.8% of the variance in streamflow (q_mean). This suggests the model has generalized extremely well to unseen data, maintaining high accuracy without overfitting to the training data. The high R² value indicates the model captures nearly all the explainable variation in streamflow based on the available predictors.\n\n\nUse the collect_predictions() function to check the predictions of the final model on the test data. This will return a tibble with the predictions for the final model.\n\ntest_predictions &lt;- collect_predictions(final_fit)\n\n\n\nUse the output of this to create a scatter plot of the predicted values vs the actual values. Use the ggplot2 package to create the plot. This plot should include (1) geom_smooth(method = “lm”) to add the linear fit of predictions and truth (2) geom_abline() to add a 1:1 line (3) nice colors via scale_color_* and (4) accurate labels.\n\nggplot(test_predictions, aes(x = q_mean, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray50\") +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Predicted vs Actual Streamflow\",\n    x = \"Actual Streamflow (q_mean)\",\n    y = \"Predicted Streamflow\",\n    subtitle = \"Random Forest Model Performance on Test Data\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Map!\n\nThis full fit can be passed to the augment() function to make predictions on the full, cleaned data. This will return a tibble with the predictions for the full data.\n\nfull_fit &lt;- fit(final_workflow, data_combined_clean)\npredictions_full &lt;- augment(full_fit, data_combined_clean)\n\n\n\nUse the mutate() function to calculate the residuals of the predictions. The residuals are the difference between the predicted values and the actual values squared.\n\npredictions_with_residuals &lt;- predictions_full %&gt;%\n  mutate(residual_squared = (q_mean - .pred)^2)\n\n\n\nUse ggplot2 to create a map of the predictions.\n\npredictions_with_residuals &lt;- predictions_with_residuals %&gt;%\n  mutate(\n    basin_code = as.numeric(substr(gauge_id, 1, 2)),\n    sub_basin = as.numeric(substr(gauge_id, 3, 5))\n  )\n\nlibrary(ggplot2)\n\nmap_predictions &lt;- ggplot(predictions_with_residuals, \n                         aes(x = basin_code, y = sub_basin, color = .pred)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Predicted\\nStreamflow\", option = \"plasma\") +\n  labs(\n    title = \"Predicted Streamflow by Basin Code\",\n    x = \"Major Basin Code\",\n    y = \"Sub-basin Code\",\n    subtitle = \"Using gauge ID components as proxy for location\"\n  ) +\n  theme_minimal()\n\n\n\nUse ggplot2 to create a map of the residuals.\n\nmap_residuals &lt;- ggplot(predictions_with_residuals, \n                        aes(x = basin_code, y = sub_basin, color = residual_squared)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_viridis_c(\n    name = \"Squared\\nResiduals\", \n    option = \"magma\", \n    trans = \"log\"\n  ) +\n  labs(\n    title = \"Model Residuals by Basin Code\",\n    x = \"Major Basin Code\",\n    y = \"Sub-basin Code\",\n    subtitle = \"Using gauge ID components as proxy for location\"\n  ) +\n  theme_minimal()\n\n\n\nUse patchwork to combine the two maps into one figure.\n\nlibrary(patchwork)\ncombined_maps &lt;- map_predictions / map_residuals\ncombined_maps"
  }
]